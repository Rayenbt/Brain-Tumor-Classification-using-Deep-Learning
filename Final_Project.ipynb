{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05efe45c-5cae-4950-80a3-6ff5dcb25749",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8e1af0-cc5b-4b60-a470-e27ab14da89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63bfb8c-de7a-47bf-a447-f848cc2997ac",
   "metadata": {},
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db938dc-2197-4bbd-8599-234b4ef7513e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'Dataset'\n",
    "subsets = ['Training','Testing'] # Dataset divided into 2 subsets: training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2615022-5207-4895-8c2e-5ee3fb2dabbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = []\n",
    "\n",
    "for subset in subsets: # goes through the 2 subsets training and testing\n",
    "    subset_path = os.path.join(dataset_path,subset)\n",
    "    diagnoses = os.listdir(subset_path) # Returns a list of diagnoses\n",
    "\n",
    "    for diag in diagnoses: # iterates over the diagnoses\n",
    "        diagnoses_folder = os.path.join(subset_path, diag )\n",
    "        for img_file in os.listdir(diagnoses_folder):\n",
    "            img_path = os.path.join(diagnoses_folder, img_file) # gets the image path within the diagnoses folder \n",
    "            with Image.open(img_path) as img: \n",
    "                width, height = img.size # Collects the width, height, and mode of the images\n",
    "                mode = img.mode\n",
    "                image_data.append({ # adds all the collected info for each image to the list\n",
    "                    'subset':subset,\n",
    "                    'diagnosis': diag,\n",
    "                    'width': width,\n",
    "                    'height': height,\n",
    "                    'mode': mode,\n",
    "                    'image': np.array(img) #image converted to numpy array\n",
    "                })\n",
    "\n",
    "\n",
    "df = pd.DataFrame(image_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9873db01-e741-4c0e-b1be-4fb5e4b67217",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e156de-ca75-4f65-9be6-55a1e9f36f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of images for training and testing for each diagnosis\n",
    "table = df.groupby(['subset', 'diagnosis']).size().unstack()\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d102ca-1a75-4e43-a9b6-60e6540b26fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df['subset'] == 'Training']\n",
    "df_test = df[df['subset'] == 'Testing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7e5281-f918-4561-b07a-6c4d2a5aac68",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = table.loc['Training'] \n",
    "testing_data= table.loc['Testing']\n",
    "\n",
    "fig, ax = plt.subplots(ncols = 3,figsize=(20,14))\n",
    "\n",
    "# training data split\n",
    "ax[0].pie(training_data, labels= training_data.index,colors = [\"#F9ED69\", \"#F08A5D\", \"#B83B5E\", \"#6A2C70\"], \n",
    "    autopct=lambda p: '{:.2f}%\\n{:,.0f}'.format(p, p * sum(df_train['diagnosis'].value_counts()) / 100),\n",
    "    textprops={'fontsize': 20})\n",
    "ax[0].set_title(\"Training\")\n",
    "\n",
    "# train test split\n",
    "ax[1].set_title(\"Train Test Split\")\n",
    "ax[1].pie(\n",
    "    [len(df_train), len(df_test)], \n",
    "    labels=['Train', 'Test'], \n",
    "    colors=['#144272', 'orange'],  # Fixed colors syntax and spelling\n",
    "    autopct=lambda p: '{:.2f}%\\n{:,.0f}'.format(p, p * (len(df_train) + len(df_test)) / 100),\n",
    "    startangle=85, \n",
    "    textprops={'fontsize': 20}\n",
    ")\n",
    "\n",
    "# testing data split\n",
    "ax[2].set_title('Testing Data')\n",
    "ax[2].pie(testing_data, labels= testing_data.index, colors=[\"#F9ED69\", \"#F08A5D\", \"#B83B5E\", \"#6A2C70\"], \n",
    "    autopct=lambda p: '{:.2f}%\\n{:,.0f}'.format(p, p * sum(df_test['diagnosis'].value_counts()) / 100),\n",
    "    textprops={'fontsize': 20 })\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d184339a-b91d-410e-ac7d-1e5acdbd45f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of image dimensions \n",
    "plt.figure(figsize=(8,5))\n",
    "sns.scatterplot(data = df, x='width',y='height', hue='diagnosis',alpha=0.6)\n",
    "plt.title('Image dimensions by diagnosis')\n",
    "plt.xlabel('Width')\n",
    "plt.ylabel('Height')\n",
    "plt.legend(title='Diagnosis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b90b33-5332-4fa5-b19e-059dd8f1ad5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the mode of images, shows inconsistency across images\n",
    "df['mode'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8fdf0e-5b45-45fc-bca7-0d1fcf63e46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number\n",
    "sns.countplot(data=df, x='mode', hue='diagnosis', palette=['royalblue', 'peru', 'red', 'green'],legend='full')\n",
    "plt.title('The number of images per image mode')\n",
    "plt.xlabel('Mode')\n",
    "plt.ylabel('Number of images')\n",
    "plt.legend(title='Mode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89aafb1a-11a6-43ab-ab56-c9b7765dbc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take 5 images from each diagnosis and visualise them, raw images without processing\n",
    "for diag in diagnoses:\n",
    "    samples = [item for item in image_data if item['diagnosis'] == diag]\n",
    "    samples = random.sample(samples, k=5)  \n",
    "    \n",
    "    plt.figure(figsize=(15, 3))\n",
    "    for i, item in enumerate(samples):\n",
    "        plt.subplot(1, 5, i + 1)\n",
    "        plt.imshow(item['image'])\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(f\"Diagnosis: {diag}\",fontsize=20)\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f593e4f-4feb-4960-b46c-ea6960324fd9",
   "metadata": {},
   "source": [
    "## Findings\n",
    "The brain tumors dataset consists of 4 different brain diagnoses: Glioma, meningioma, pituitary and healthy (known as notumor). There are 7023 images; 5645 for training (80%) and 1411 for testing (20%) divided closely between the four diagnoses, also each subset is well divided among the 4 classes with a maximum of 28.35% for healthy brain and a minimum of 22.89% for glioma in the training set, with nearly similar results for the testing set indicating a good class balance to start with.\n",
    "\n",
    "However, the image dimensions by diagnosis chart shows a huge variance in image dimensions that needs to be handled as part of the data preparation, we need to choose a common image size for all the images which may affect the performance of the model as certain parts of the brain may/may not be taken when resizing especially when it comes to locating tumors within the brain. Also the fact that some images have different color modes (3926 RGB and 3093 Greyscale) so we need to ensure consistency among shapes and channels to avoid potential issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8056fa54-51ce-4000-89a3-c0c6baf08969",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e797c9-c92f-460e-afcc-710d3dcea4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'Dataset/Training' # train images directory\n",
    "test_dir = 'Dataset/Testing'  # test images directory\n",
    "batch_size= 32 # define batch size\n",
    "image_size = 224 # define image size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0883fae-ee38-4f57-bee4-ee159cdd72ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load training data using image_dataset_from_directory\n",
    "#source: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir,                                     \n",
    "    label_mode= 'int',                             # Labels are encoded as a integers\n",
    "    validation_split= 0.2,                         # 20% of the training data will be used for training\n",
    "    color_mode = 'rgb',                            # Load all images in RGB mode \n",
    "    batch_size = batch_size,                       # number of images processed in each batch\n",
    "    image_size = (image_size,image_size),          # image size is 224x224 (this will fix the imbalance we plotted earlier by using a uniform size)\n",
    "    seed = 111,\n",
    "    subset = 'training'                            # Training subset\n",
    ")\n",
    "\n",
    "#load validation data using image_dataset_from_directory\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    label_mode= 'int',\n",
    "    validation_split= 0.2,\n",
    "    color_mode = 'rgb',\n",
    "    batch_size = batch_size,\n",
    "    image_size = (image_size,image_size),\n",
    "    shuffle = False,                                # Validation data isn't shuffled\n",
    "    seed = 111,\n",
    "    subset = 'validation'                           # Validation subset\n",
    ")\n",
    "\n",
    "#load validation data using image_dataset_from_directory\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    label_mode= 'int',\n",
    "    color_mode = 'rgb',\n",
    "    batch_size = batch_size,\n",
    "    image_size = (image_size, image_size)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a9828a-edeb-4090-884f-b4fae36c2471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "# Source: https://www.tensorflow.org/api_docs/python/tf/keras/layers\n",
    "data_augmentation = Sequential([\n",
    "    layers.RandomFlip('horizontal'),                                                             # Randomly flips images horizontally\n",
    "    layers.RandomRotation(0.03,fill_mode='constant'),                                            # Randomly rotates images by ±10 degrees\n",
    "    layers.RandomContrast(0.1),                                                                  # Randomly adjusts contrast by ±10%\n",
    "    layers.RandomZoom(height_factor=0.01, width_factor=0.05),                                    # Randomly zooms in/out up to 1% vertically and 5% horizontally\n",
    "    layers.RandomTranslation(height_factor=0.0015, width_factor=0.0015, fill_mode='constant'),   # Randomly shifts the image by 0.15% of the height and width\n",
    "   \n",
    "])\n",
    "\n",
    "rescale = layers.Rescaling(1./255)  # Rescale pixel values to [0, 1] range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ec9668-053c-4332-afd0-faba10eaebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://www.tensorflow.org/tutorials/images/data_augmentation\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE # Allows TensorFlow to automatically choose the optimal number of parallel threads to improve performance\n",
    "\n",
    "# training data augmented and rescaled (Images were already resized when loaded so we don't need to resize again)\n",
    "train_ds = train_ds.map(\n",
    "    lambda x, y: (rescale(data_augmentation(x,training= True)), y),\n",
    "    num_parallel_calls= AUTOTUNE\n",
    ")\n",
    "\n",
    "# Validation data rescaled only\n",
    "val_ds = val_ds.map(\n",
    "    lambda x, y: (rescale(x), y),\n",
    "    num_parallel_calls= AUTOTUNE\n",
    ")\n",
    "\n",
    "# Testing data rescaled only\n",
    "test_ds = test_ds.map(\n",
    "    lambda x, y: (rescale(x), y),\n",
    "    num_parallel_calls= AUTOTUNE\n",
    ")\n",
    "\n",
    "# Allows later elements to be prepared while the current element is being processed which improves performance\n",
    "train_ds_preprocessed = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds_preprocessed = val_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds_preprocessed = test_ds.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111f860b-669a-4545-bc43-c0dbdf15ccbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_augmentation(data, diagnoses, num_samples):\n",
    "    \n",
    "    # empty dictionary that will contain an image per diagnosis\n",
    "    diagnoses_images = {diag: None for diag in range(len(diagnoses))}\n",
    "    \n",
    "    # Unbatch data and get individual images\n",
    "    for image, label in data.unbatch().shuffle(1000):\n",
    "        label = int(label.numpy()) # convert tensorflow label to a python integer\n",
    "        if diagnoses_images[label] is None:  # If we didn't store an image\n",
    "            img = (image.numpy() * 255).astype(np.uint8) # convert image to an array and we scale it back to 0-255 (img_size = 224)\n",
    "            diagnoses_images[label] = img # we add the image to the dictionary\n",
    "       \n",
    "            \n",
    "    \n",
    "    # Create visualization grid\n",
    "    num_diagnoses = len(diagnoses) \n",
    "    fig, axes = plt.subplots(\n",
    "        num_diagnoses,\n",
    "        num_samples + 1,\n",
    "        figsize=(2.5*(num_samples+1), 2.5*num_diagnoses))\n",
    "    \n",
    "    for class_idx in range(num_diagnoses):\n",
    "        # Original image taken earlier\n",
    "        orig_img = diagnoses_images[class_idx]\n",
    "        # Add it in the first column of each row \n",
    "        axes[class_idx, 0].imshow(orig_img)\n",
    "        axes[class_idx, 0].set_title(f\"Original\\n{diagnoses[class_idx]}\")\n",
    "        axes[class_idx, 0].axis('off')\n",
    "        \n",
    "        for aug_idx in range(1, num_samples+1):\n",
    "            # We augment the original images \n",
    "            aug_img = data_augmentation(tf.expand_dims(orig_img, 0))\n",
    "            aug_img = aug_img[0].numpy().astype('uint8')\n",
    "\n",
    "            # then we add it to the visualisation grid\n",
    "            axes[class_idx, aug_idx].imshow(aug_img)\n",
    "            axes[class_idx, aug_idx].set_title(f\"Augmented image {aug_idx}\")\n",
    "            axes[class_idx, aug_idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf7c4b6-9897-49de-a7e8-157ce1104323",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_augmentation(\n",
    "    data=train_ds,\n",
    "    diagnoses = diagnoses,  \n",
    "    num_samples=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861aa665-58cc-4d3c-8cce-0acd300fea30",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cf6da1-128e-4f55-9417-aabd5f5ce259",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (224,224,3)\n",
    "num_classes= 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcab7bfa-285f-46e6-aba6-0ca5aa146db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_minimal_cnn():\n",
    "    model = Sequential([\n",
    "        Conv2D(16, (3,3), activation='relu', input_shape=input_shape),\n",
    "        MaxPooling2D((2,2)),\n",
    "        \n",
    "        Conv2D(32, (3,3), activation='relu'),\n",
    "        MaxPooling2D((2,2)),\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=1e-3),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Build & train briefly for a small number of epochs (e.g., 5)\n",
    "model_basic = build_minimal_cnn()\n",
    "history_v1 = model_basic.fit(\n",
    "    train_ds_preprocessed, \n",
    "    validation_data=val_ds_preprocessed, \n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "# Evaluate quickly on validation\n",
    "val_loss_v1, val_acc_v1 = model_basic.evaluate(test_ds_preprocessed)\n",
    "print(f\"Testing Accuracy: {val_acc_v1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py310 - TensorFlow)",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
